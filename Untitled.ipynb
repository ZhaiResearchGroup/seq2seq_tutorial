{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Translation (with attention)\n",
    "\n",
    "The character level RNN was a great intro to RNN's and pytorch in general but its not really where we'd like to go.\n",
    "\n",
    "There were a few key (abilt somewhat obvious) problems with it for our application.\n",
    "\n",
    "  1. We're operating at two low of a granularity - the fundemental unit of lanuage is the word, not the character (at least in english)\n",
    "  2. We have no control over the content we genereated beyond the category\n",
    "\n",
    "Our aim here will be to implement a basic version of the Machine Translation task: converting a sentence from French to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Imports!\n",
    "from __future__ import unicode_literals, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this will end up being false for everyone execpt duke\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data representation\n",
    "\n",
    "\n",
    "While representing every letter in a language was feasible, representing every word is far from. While there are sophisticated ways to solve this problem, for the moment we'll just use a one-hot vector of most common few thousand words in the language to represent the complete possible vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the char-rnn example we used one-hot vectors to represent each letter in a the word. Since we're changing our language unit (from chars to words) we're going to need a new representation.\n",
    "\n",
    "However, before we can even worry about how we repsent each word, we need to be able to uniquely identify each word by some numeric value. \n",
    "\n",
    "To do this in a clean way (since we now have more than one set of language units (english vs french words)) we'll create a class to handle language operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0 # Start Of Sentence\n",
    "EOS_token = 1 # End   \"      \"\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "            self.num_words = 2\n",
    " \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '): # split on space char\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words # next largest number\n",
    "            self.word2count[word] = 1 # first occurance\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Data cleaning shit from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_2_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def clean_string(s):\n",
    "    s = unicode_2_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and clean\n",
    "    pairs = [[clean_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set trimming\n",
    "Sometimes we are either limited by computation or we only care about a subproblem within a data set - at times like this its important to intelligently limit the data we operate on.\n",
    "\n",
    "In the compuation space, if youre trying to see if an approach is viable you could just arbitraily limit yourself to only look at 10k examples or something but you'd just get a shitty model that potentially wouldnt even validate your appoarch as resonable.\n",
    "\n",
    "A smarter trick (and the one we'll apply here) is to cherry pick data that has something in common, and just try to learn the relationship expressed within that subset. In this example we're gonna choose to train a model that knows how to translate to the form \"He is...\" or \"We are...\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [p for p in pairs if filter_pair(p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentences\n",
      "Trimed to 10853 sens\n",
      "Counting words...\n",
      "fra 4489\n",
      "eng 2925\n",
      "['c est un bleu .', 'he s a freshman .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentences\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimed to %s sens\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(input_lang.name, input_lang.num_words)\n",
    "    print(output_lang.name, output_lang.num_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network\n",
    "\n",
    "##  sequence to sequence networks\n",
    "\n",
    "So we have an idea of what RNN's are from the char-rnn. What's the diffrence between those and these sequence to sequence models?\n",
    "\n",
    "A seq2seq network is actually a joint model that combines two recurrent networks in an encoder-decoder architecture...\n",
    "\n",
    "![alt text](encdec.jpg \"Logo Title Text 1\")\n",
    "\n",
    "where both the encoder and the decoder is a RNN.\n",
    "\n",
    "In such an architecture the encoder network converts a sentence into a \"meaning\" vector (think semantic embeding) and this vector is the passed to the decoder which converts the embedding to its translation in the choosen language.\n",
    "\n",
    "A major advantage of this sort of architecture is that it saves us from having to make our translation a word to word conversion and thereby keep the same order of words and length of sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is an rnn that generates the \"meaning\" or context vector to feed the decoder by reading each element in the input sequence.\n",
    "\n",
    "Given the seq \"This is a cat\" (A B C D) and a goal seq \"yeh ek billi hai\" (W X Y Z):\n",
    "\n",
    "![alt text](input_output.jpg \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "the encoder will start with some default initalized hidden state and then update at each timestep of the input sequence. In the simiplest version of the seq2seq model we will just use the final vector produced by the encoder to fully specify the context.\n",
    "\n",
    "\n",
    "![alt text](simple_enc_dec.jpg \"Logo Title Text 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Architecture\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
