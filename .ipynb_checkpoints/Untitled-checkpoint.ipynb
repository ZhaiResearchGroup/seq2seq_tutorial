{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Translation (with attention)\n",
    "\n",
    "The character level RNN was a great intro to RNN's and pytorch in general but its not really where we'd like to go.\n",
    "\n",
    "There were a few key (abilt somewhat obvious) problems with it for our application.\n",
    "\n",
    "  1. We're operating at two low of a granularity - the fundemental unit of lanuage is the word, not the character (at least in english)\n",
    "  2. We have no control over the content we genereated beyond the category\n",
    "\n",
    "Our aim here will be to implement a basic version of the Machine Translation task: converting a sentence from French to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  sequence to sequence networks\n",
    "\n",
    "Sequence to sequence network are actually a joint model that combines two recurrent networks in an encoder-decoder architecture.\n",
    "\n",
    "![alt text](encdec.jpg \"Logo Title Text 1\")\n",
    "\n",
    "In such an architecture the encoder network converts a sentence into a \"meaning\" vector (think semantic embeding) and this vector is the passed to the decoder which converts the embedding to its translation in the choosen language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Imports!\n",
    "from __future__ import unicode_literals, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this will end up being false for everyone execpt duke\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data representation\n",
    "\n",
    "\n",
    "While representing every letter in a language was feasible, representing every word is far from. While there are sophisticated ways to solve this problem, for the moment we'll just use a one-hot vector of most common few thousand words in the language to represent the complete possible vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the char-rnn example we used one-hot vectors to represent each letter in a the word. Since we're changing our language unit (from chars to words) we're going to need a new representation.\n",
    "\n",
    "However, before we can even worry about how we repsent each word, we need to be able to uniquely identify each word by some numeric value. \n",
    "\n",
    "To do this in a clean way (since we now have more than one set of language units (english vs french words)) we'll create a class to handle language operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0 # Start Of Sentence\n",
    "EOS_token = 1 # End   \"      \"\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "            self.num_words = 2\n",
    " \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '): # split on space char\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words # next largest number\n",
    "            self.word2count[word] = 1 # first occurance\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Data cleaning shit from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
